{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "723e22aa",
   "metadata": {},
   "source": [
    "# **Desafío #3 – Proceso batch hacia un Data Lake en Azure**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43177eee",
   "metadata": {},
   "source": [
    "## **3.1 Introducción** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36bb2f3",
   "metadata": {},
   "source": [
    "Este notebook documenta el desarrollo del Desafío #3 del componente de Ingeniería de Datos + Cloud.\n",
    "\n",
    "En los desafíos anteriores se construyó un modelo sintético de una organización (departamentos, puestos de trabajo y empleados) y se persistieron los datos resultantes en formato CSV y Parquet dentro del proyecto local, en la ruta `01_data_cloud/data/raw`.\n",
    "\n",
    "El objetivo de este tercer desafío es diseñar e implementar un proceso batch que migre esos datos hacia un entorno de almacenamiento en la nube, utilizando un servicio tipo Data Lake en Azure. El enunciado de la prueba permite utilizar una base de datos SQL/NoSQL, un Data Warehouse o un bucket analítico dentro de un Datalake. En esta solución se selecciona como destino un **Azure Storage Account con capacidades de Data Lake (Blob Storage / ADLS Gen2)**, aprovechando la capa gratuita disponible en la suscripción académica.\n",
    "\n",
    "El proceso batch se implementa como un script de Python idempotente que:\n",
    "\n",
    "- lee los archivos CSV y Parquet ubicados en `data/raw`,  \n",
    "- organiza los datos en una estructura de carpetas propia de una capa *raw* de Data Lake,  \n",
    "- y los carga a un contenedor de Azure Storage preparado para ser consumido por procesos analíticos posteriores.  \n",
    "\n",
    "Este enfoque cumple con el requerimiento del Desafío #3 y se alinea con prácticas habituales de ingeniería de datos en arquitecturas modernas basadas en Data Lake.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd76bb4",
   "metadata": {},
   "source": [
    "## **3.2 Configuración de rutas y carga del `.env`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a98779c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook directory : /Users/capeto/Library/CloudStorage/OneDrive-Personal/GitHub/mvm-technical-challenge/01_data_cloud/docs\n",
      "Project root       : /Users/capeto/Library/CloudStorage/OneDrive-Personal/GitHub/mvm-technical-challenge/01_data_cloud\n",
      "Src path           : /Users/capeto/Library/CloudStorage/OneDrive-Personal/GitHub/mvm-technical-challenge/01_data_cloud/src\n",
      ".env path          : /Users/capeto/Library/CloudStorage/OneDrive-Personal/GitHub/mvm-technical-challenge/01_data_cloud/.env\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Directorio del notebook: 01_data_cloud/docs\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir.parent\n",
    "\n",
    "src_path = project_root / \"src\"\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.append(str(src_path))\n",
    "\n",
    "# Cargar variables del archivo .env ubicado en 01_data_cloud\n",
    "env_path = project_root / \".env\"\n",
    "load_dotenv(env_path)\n",
    "\n",
    "print(\"Notebook directory :\", notebook_dir)\n",
    "print(\"Project root       :\", project_root)\n",
    "print(\"Src path           :\", src_path)\n",
    "print(\".env path          :\", env_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435ee68b",
   "metadata": {},
   "source": [
    "## **3.3 Arquitectura del proceso batch**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d644f0d",
   "metadata": {},
   "source": [
    "El proceso batch diseñado para este desafío sigue la siguiente arquitectura lógica:\n",
    "\n",
    "1. **Fuente local:** Los archivos generados en los Desafíos #1 y #2 se encuentran en:\n",
    "\n",
    "   ```text\n",
    "   01_data_cloud/data/raw\n",
    "       departments.csv / departments.parquet\n",
    "       job_positions.csv / job_positions.parquet\n",
    "       employees.csv / employees.parquet\n",
    "   ```\n",
    "\n",
    "2. **Proceso batch (Python):** Un script ubicado en `src/batch_etl/org_batch_etl.py` implementa la lógica de:\n",
    "\n",
    "   - descubrimiento de archivos en `data/raw`,  \n",
    "   - construcción de rutas destino en el Data Lake,  \n",
    "   - y carga de cada archivo a un contenedor de Azure Blob Storage.  \n",
    "\n",
    "   El proceso es **idempotente**: la re-ejecución con los mismos datos sobrescribe los archivos existentes, evitando duplicados.\n",
    "\n",
    "3. **Destino en la nube: Data Lake en Azure:** El destino es un contenedor de Azure Storage, por ejemplo `org-raw`, organizado con una estructura de prefijos que refleja una capa raw y una versión lógica del modelo:\n",
    "\n",
    "   ```text\n",
    "   container: org-raw\n",
    "     └── org_data/\n",
    "         └── v1/\n",
    "             ├── departments/\n",
    "             │   ├── departments.csv\n",
    "             │   └── departments.parquet\n",
    "             ├── job_positions/\n",
    "             │   ├── job_positions.csv\n",
    "             │   └── job_positions.parquet\n",
    "             └── employees/\n",
    "                 ├── employees.csv\n",
    "                 └── employees.parquet\n",
    "   ```\n",
    "\n",
    "Esta estructura facilita el consumo posterior desde motores de análisis (Spark, Synapse, Databricks, etc.) y mantiene claramente identificada la versión del modelo sintético (`v1`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.4 Configuración de acceso a Azure Storage**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a1410d",
   "metadata": {},
   "source": [
    "El acceso al Storage Account se gestiona mediante variables de entorno, evitando exponer credenciales en el código fuente. El proceso batch utiliza las siguientes variables:\n",
    "\n",
    "- **`AZURE_STORAGE_CONNECTION_STRING`:** Connection string completo del Storage Account (incluye `AccountName` y `AccountKey`).  \n",
    "\n",
    "- **`AZURE_STORAGE_CONTAINER_RAW`:** Nombre del contenedor donde se almacenarán los datos raw. Valor por defecto utilizado en el script: `org-raw`.  \n",
    "\n",
    "- **`AZURE_STORAGE_BASE_PREFIX`:** Prefijo base dentro del contenedor para la versión actual del modelo. Valor por defecto utilizado en el script: `org_data/v1`.  \n",
    "\n",
    "Estas variables pueden definirse a nivel de sistema operativo o mediante un archivo `.env` que no se versiona en el repositorio. El script batch lee dichas variables en tiempo de ejecución y falla explícitamente si la cadena de conexión no se encuentra configurada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "554d5ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando proceso batch de carga a Data Lake...\n",
      "Run timestamp (UTC): 2025-12-06T15:36:07.788942\n",
      "Directorio local de origen: /Users/capeto/Library/CloudStorage/OneDrive-Personal/GitHub/mvm-technical-challenge/01_data_cloud/data/raw\n",
      "Contenedor destino: org-raw\n",
      "Prefijo base en el contenedor: org_data/v1\n",
      "------------------------------------------------------------\n",
      "Subido: job_positions.parquet -> org_data/v1/job_positions/job_positions.parquet\n",
      "Subido: job_positions.csv -> org_data/v1/job_positions/job_positions.csv\n",
      "Subido: departments.csv -> org_data/v1/departments/departments.csv\n",
      "Subido: departments.parquet -> org_data/v1/departments/departments.parquet\n",
      "Subido: employees.parquet -> org_data/v1/employees/employees.parquet\n",
      "Subido: employees.csv -> org_data/v1/employees/employees.csv\n",
      "------------------------------------------------------------\n",
      "Proceso batch completado.\n"
     ]
    }
   ],
   "source": [
    "from batch_etl.org_batch_etl import run_batch_upload\n",
    "\n",
    "run_batch_upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d28536a",
   "metadata": {},
   "source": [
    "## **3.5 Implementación del proceso batch en `src/batch_etl/org_batch_etl.py`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6764a69",
   "metadata": {},
   "source": [
    "La lógica del proceso batch se implementa en el módulo `src/batch_etl/org_batch_etl.py`. Este módulo:\n",
    "\n",
    "- resuelve las rutas locales del proyecto (`data/raw`),  \n",
    "\n",
    "- carga la configuración de Azure Storage desde el entorno,  \n",
    "\n",
    "- identifica los archivos CSV y Parquet a cargar,  \n",
    "\n",
    "- construye las rutas destino dentro del contenedor,  \n",
    "\n",
    "- y realiza la carga de cada archivo utilizando el SDK oficial `azure-storage-blob`.\n",
    "\n",
    "A continuación se muestra el contenido esencial del módulo para referencia técnica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "070e3ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blobs en el contenedor 'org-raw':\n",
      " - org_data\n",
      " - org_data/v1\n",
      " - org_data/v1/departments\n",
      " - org_data/v1/departments/departments.csv\n",
      " - org_data/v1/departments/departments.parquet\n",
      " - org_data/v1/employees\n",
      " - org_data/v1/employees/employees.csv\n",
      " - org_data/v1/employees/employees.parquet\n",
      " - org_data/v1/job_positions\n",
      " - org_data/v1/job_positions/job_positions.csv\n",
      " - org_data/v1/job_positions/job_positions.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(project_root / \".env\")  # por si el kernel se reinició\n",
    "\n",
    "conn_str = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "container_name = os.getenv(\"AZURE_STORAGE_CONTAINER_RAW\", \"org-raw\")\n",
    "\n",
    "blob_service = BlobServiceClient.from_connection_string(conn_str)\n",
    "container_client = blob_service.get_container_client(container_name)\n",
    "\n",
    "print(f\"Blobs en el contenedor '{container_name}':\")\n",
    "for blob in container_client.list_blobs():\n",
    "    print(\" -\", blob.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452bd5f0",
   "metadata": {},
   "source": [
    "## **3.6 Conclusión del Desafío #3**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0dad15",
   "metadata": {},
   "source": [
    "El proceso descrito y ejecutado en este notebook completa el Desafío #3 de la prueba técnica:\n",
    "\n",
    "- Los datos sintéticos generados y persistidos localmente en formatos CSV y Parquet se migran a un **Data Lake en Azure** utilizando un Storage Account con capacidades de Blob Storage / ADLS Gen2.\n",
    "- La carga se realiza mediante un proceso batch implementado en Python, configurado por variables de entorno, idempotente y alineado con una estructura de carpetas propia de una capa *raw* versionada (`org_data/v1`).\n",
    "- La solución es coherente con arquitecturas de datos contemporáneas y prepara el terreno para la creación de vistas, consultas y APIs en los desafíos posteriores.\n",
    "\n",
    "Con esta etapa, la cadena de valor de la solución de Ingeniería de Datos + Cloud se extiende desde la generación de datos sintéticos hasta su almacenamiento analítico en la nube.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198c364d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
