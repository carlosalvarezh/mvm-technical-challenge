{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a1c1ef6",
   "metadata": {},
   "source": [
    "# **1. Contexto general de la prueba técnica**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0805381",
   "metadata": {},
   "source": [
    "La prueba técnica de MVM consiste en diseñar e implementar una solución integral de gestión de datos, enfocándose en información clave del ámbito organizacional: departamentos, puestos y empleados. Además de los aspectos técnicos, el desafío busca demostrar la habilidad para construir una arquitectura consistente que conecte la creación y almacenamiento de datos con su análisis y presentación mediante servicios modernos, siguiendo las mejores prácticas de ingeniería de datos en entornos en la nube.\n",
    "\n",
    "En un contexto real de negocio, una compañía como MVM necesita consolidar y gobernar sus datos internos para responder preguntas que van desde la planificación de talento y de capacidad operativa hasta la evaluación de desempeño y la optimización de estructuras organizacionales. La prueba recrea, a pequeña escala, ese escenario: se parte de datos que representan la organización interna y se solicita diseñar una solución que permita almacenarlos, transformarlos, consultarlos y exponerlos de forma segura y escalable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9180c25f",
   "metadata": {},
   "source": [
    "# **2. Sentido del proyecto de Ingeniería de Datos + Cloud**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6682110",
   "metadata": {},
   "source": [
    "El primer componente de la prueba, enfocado en Ingeniería de Datos + Cloud, está concebido como un **mini ecosistema de datos** que debe integrar varios elementos clave:\n",
    "\n",
    "* Generación controlada de datos sintéticos que simulan la realidad de una organización (departamentos, cargos y empleados).  \n",
    "\n",
    "* Almacenamiento estructurado en formatos adecuados para análisis y procesamiento masivo.  \n",
    "\n",
    "* Migración de esos datos a una plataforma de datos en la nube (base de datos relacional, data warehouse o data lake).  \n",
    "\n",
    "* Definición de vistas o consultas que representen casos de uso analíticos concretos.  \n",
    "\n",
    "* Exposición de la información mediante una API REST, que actúe como capa de servicio reutilizable para aplicaciones internas o externas.  \n",
    "\n",
    "* Opcionalmente, empaquetamiento de la solución en contenedores para facilitar su despliegue y operación.  \n",
    "\n",
    "En conjunto, este flujo refleja el tipo de soluciones que un ingeniero de datos debe entregar en una organización que trabaja con arquitecturas modernas de datos y servicios, donde la trazabilidad de extremo a extremo es tan importante como la calidad técnica de cada componente aislado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b7185f",
   "metadata": {},
   "source": [
    "# **3. Objetivos del proyecto**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96b2746",
   "metadata": {},
   "source": [
    "Desde la perspectiva de la prueba, el proyecto de Ingeniería de Datos + Cloud persigue, como mínimo, los siguientes objetivos:\n",
    "\n",
    "1. **Demostrar dominio en modelado y generación de datos**: diseñar un conjunto de datos sintético consistente, con relaciones claras entre departamentos, puestos y empleados, que pueda utilizarse como base para análisis posteriores.  \n",
    "\n",
    "2. **Validar criterios de elección de formatos y de almacenamiento**: justificar la elección de formatos de archivo (CSV, Parquet, etc.) y de servicios de almacenamiento en la nube, considerando aspectos como la compresión, el esquema, el costo y la facilidad de integración con otras herramientas.  \n",
    "\n",
    "3. **Evidenciar capacidades de integración y orquestación**: implementar un proceso batch que lea los archivos generados, aplique las transformaciones mínimas necesarias y cargue los datos en una plataforma de datos (por ejemplo, una base de datos SQL en la nube), cuidando la consistencia de tipos, claves y nombres.  \n",
    "\n",
    "4. **Conectar el modelo de datos con casos de uso analíticos**: construir vistas o consultas que respondan a preguntas que un área de negocio podría formular, como la distribución de empleados por departamento, el análisis de puestos, métricas agregadas, entre otras.  \n",
    "\n",
    "5. **Exponer los datos como servicio**: desarrollar una API REST que permita consumir estas vistas o consultas de forma controlada, preparando el terreno para aplicaciones de reporting, autoservicio analítico o integración con otros sistemas.  \n",
    "\n",
    "6. **Mostrar sensibilidad a aspectos de operación y escalabilidad**: empaquetar la API en contenedores y describir, al menos de forma conceptual, cómo la solución podría escalar y operar en un entorno productivo, incluyendo la explicitación de la deuda técnica que se asume en el contexto de la prueba.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7dd3ab",
   "metadata": {},
   "source": [
    "# **4. Resultados esperados y alineación con MVM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bce7680",
   "metadata": {},
   "source": [
    "El resultado esperado no es únicamente un conjunto de scripts o una API que funcione, sino una **solución coherente y bien documentada**, capaz de ser entendida, ejecutada y extendida por otros profesionales. La documentación debe reflejar las decisiones tomadas, las alternativas consideradas y las limitaciones reconocidas, de manera que quien revise el repositorio pueda seguir el hilo completo de la solución: desde la generación de los datos hasta su consumo a través de la API.\n",
    "\n",
    "En términos de alineación con MVM, este proyecto destaca habilidades clave para el rol de ingeniería de datos en una organización de servicios energéticos y tecnológicos: la capacidad de abstraer un dominio de negocio, diseñar modelos de datos apropiados, escoger tecnologías de almacenamiento y procesamiento en la nube, y crear servicios que permitan a otras áreas de la empresa utilizar esos datos de manera confiable. La evaluación, por tanto, no se concentra únicamente en la habilidad de programar, sino en la forma en que se piensa, diseña y articula una solución de datos integral.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e30ffa2",
   "metadata": {},
   "source": [
    "# **5. Alcance y decisiones de diseño del Proyecto de Ingeniería de Datos + Cloud**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1d772a",
   "metadata": {},
   "source": [
    "El desarrollo de este proyecto exige definir desde el inicio un conjunto de decisiones que delimitan el alcance, las tecnologías a emplear y las condiciones bajo las cuales se implementará la solución. La claridad en esta etapa es determinante: permite construir un proyecto coherente, reproducible y alineado con prácticas modernas de ingeniería de datos, evitando improvisaciones y manteniendo la trazabilidad conceptual desde el diseño hasta la entrega final."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9daa57",
   "metadata": {},
   "source": [
    "## **5.1 Alcance funcional**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c9df27",
   "metadata": {},
   "source": [
    "El proyecto cubrirá todo el ciclo de datos, desde su generación sintética hasta su presentación a través de un servicio de consulta. En términos prácticos, esto implica:\n",
    "\n",
    "1. **Generación de la data fuente**: Se simularán tres entidades principales del dominio organizacional: departamentos, puestos y empleados. Los datos serán coherentes entre sí, con relaciones lógicas entre las tablas, valores plausibles y una estructura bien fundamentada.  \n",
    "\n",
    "2. **Persistencia en formato de archivo**: Los datos generados se almacenarán en formatos estructurados (CSV y/o Parquet). Esto permitirá justificar distintas decisiones de formato y preparar el terreno para la etapa de ingestión en la nube.  \n",
    "\n",
    "3. **Migración hacia un servicio en la nube**: Se implementará un proceso batch que lea los archivos, aplique transformaciones mínimas y cargue la información en una base de datos SQL en Azure, aprovechando la capa gratuita disponible. Esta base de datos será el repositorio estructurado final de los datos.  \n",
    "\n",
    "4. **Construcción de vistas analíticas**: En la base de datos se crearán consultas o vistas que recojan preguntas típicas de negocio, como la distribución de empleados por departamento, los salarios promedio, el análisis de puestos u otras métricas relacionadas.  \n",
    "\n",
    "5. **Exposición de los datos mediante API REST**: Se construirá una API con FastAPI que consuma estas vistas desde Azure SQL y entregue resultados limpios y accesibles para un eventual front, un analista de datos u otro sistema interno.  \n",
    "\n",
    "6. **Contenerización y consideraciones de despliegue**: La API se empaquetará en un contenedor Docker. Según la disponibilidad de créditos y costos, se dejará definido o implementado un despliegue básico en Azure, explicitando las decisiones y la deuda técnica asociada.  \n",
    "\n",
    "Este alcance integra todos los pasos de una arquitectura moderna de datos, lo que permite demostrar competencias tanto técnicas como de diseño.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b53b876",
   "metadata": {},
   "source": [
    "## **5.2 Decisiones de tecnología**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ddf010",
   "metadata": {},
   "source": [
    "Dado que la prueba no impone herramientas específicas, la selección tecnológica se realiza considerando la coherencia, el costo y el nivel de madurez de cada componente. Las decisiones adoptadas para este proyecto son:  \n",
    "\n",
    "1. **Lenguaje principal: Python** Su ecosistema permite cubrir la generación de datos, el proceso de migración en batch, la conexión con servicios en la nube y el desarrollo de la API. La solución se desarrollará en un único entorno virtual controlado, evitando la fragmentación.  \n",
    "\n",
    "2. **Almacenamiento en la nube: Azure Blob Storage** Se empleará una Storage Account como Data Lake ligero, donde se alojarán los archivos CSV/Parquet. Esta elección separa claramente la capa de almacenamiento de la capa relacional y mantiene la compatibilidad con ETL, análisis y servicios de Azure.  \n",
    "\n",
    "3. **Base de datos relacional: Azure SQL Database** La capa SQL permitirá estructurar los datos y elaborar consultas analíticas. Su opción serverless y precios ajustables la convierten en una alternativa viable para la prueba y el presupuesto de créditos gratuitos.  \n",
    "\n",
    "4. **Framework para la API: FastAPI** Se selecciona por su claridad conceptual, su velocidad y su compatibilidad con estándares modernos (OpenAPI). Además, su estructura modular facilita la contenerización y el despliegue.  \n",
    "\n",
    "5. **Contenedores: Docker** La contenerización asegura que la API sea portable, ejecutable en entornos homogéneos y desplegable en futuros servicios en la nube. El uso de Docker también demuestra madurez operativa y sensibilidad hacia las prácticas de DevOps.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa589e7e",
   "metadata": {},
   "source": [
    "## **5.3 Supuestos adoptados**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417228a6",
   "metadata": {},
   "source": [
    "Para mantener el enfoque y evitar esfuerzos que no agregan valor dentro del contexto de la prueba, se establecen algunos supuestos prácticos:  \n",
    "\n",
    "* La cantidad de datos sintéticos será suficiente para demostrar la consistencia y un rendimiento razonable, sin necesidad de generar millones de registros.  \n",
    "\n",
    "* La arquitectura se orientará a un escenario de prueba y validación, no a un flujo de producción complejo con orquestadores, pipelines automatizados o monitoreo avanzado.  \n",
    "\n",
    "* Se priorizará la claridad conceptual por encima de la complejidad innecesaria: la solución debe ser robusta, entendible y replicable, no excesiva.  \n",
    "\n",
    "* Cuando existan varias alternativas equivalentes, se seleccionará la opción más alineada con las prácticas actuales de ingeniería de datos en entornos corporativos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a48d2d",
   "metadata": {},
   "source": [
    "## **5.4 Resultado esperado**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d53c98a",
   "metadata": {},
   "source": [
    "Al finalizar este proyecto se tendrá un repositorio completamente funcional que contenga:\n",
    "\n",
    "* Datos sintéticos bien diseñados.  \n",
    "\n",
    "* Archivos CSV/Parquet almacenados localmente y en Blob Storage.  \n",
    "\n",
    "* Scripts ETL para la migración a Azure SQL.  \n",
    "\n",
    "* Vistas analíticas generadas en la base de datos.  \n",
    "\n",
    "* Una API FastAPI capaz de consultar dichas vistas.  \n",
    "\n",
    "* Un contenedor Docker listo para su ejecución o despliegue.  \n",
    "\n",
    "* Documentación extensa que explique el razonamiento detrás de cada decisión.  \n",
    "\n",
    "El resultado final refleja una solución integral en la que cada componente cumple un propósito y todas las piezas encajan en un flujo natural de ingeniería de datos, demostrando un nivel de entendimiento senior y una visión holística de la arquitectura.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b508a616",
   "metadata": {},
   "source": [
    "# **6. Diseño de la estructura del repositorio**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3692438a",
   "metadata": {},
   "source": [
    "Un proyecto de ingeniería de datos que integra la generación de información, el almacenamiento, los procesos batch, las bases de datos, las vistas analíticas, las API y los contenedores requiere una organización clara que permita aislar responsabilidades, facilitar la navegación y garantizar la trazabilidad de cada componente. Antes de crear carpetas o escribir cualquier línea de código, es fundamental definir explícitamente cómo se estructurará el repositorio y por qué.\n",
    "\n",
    "La estructura propuesta cumple con criterios de modularidad, escalabilidad y mantenibilidad, de modo que cualquier persona pueda comprender la solución sin depender de explicaciones externas. Del mismo modo, esta organización permite crecer en el futuro, incorporando nuevos procesos, flujos o componentes sin necesidad de reestructurar el proyecto desde cero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de14f3d4",
   "metadata": {},
   "source": [
    "## **6.1 Estructura general propuesta**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b08426",
   "metadata": {},
   "source": [
    "El repositorio se dividirá en dos grandes áreas, correspondientes a los dos componentes de la prueba técnica:\n",
    "\n",
    "```\n",
    "mvm-data-challenge/\n",
    "│\n",
    "├── 01_data_cloud/\n",
    "│   ├── data/\n",
    "│   │   ├── raw/\n",
    "│   │   └── processed/\n",
    "│   ├── src/\n",
    "│   │   ├── data_generation/\n",
    "│   │   ├── batch_etl/\n",
    "│   │   └── api/\n",
    "│   ├── docker/\n",
    "│   ├── docs/\n",
    "│   ├── .env.example\n",
    "│   └── requirements.txt\n",
    "│\n",
    "├── 02_bi_northwind/\n",
    "│   ├── pbix/\n",
    "│   ├── docs/\n",
    "│   └── notes/\n",
    "│\n",
    "└── README.md\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48937623",
   "metadata": {},
   "source": [
    "## **6.2 Explicación de cada sección**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d98c98d",
   "metadata": {},
   "source": [
    "**`01_data_cloud/`** \n",
    "\n",
    "Contendrá todo el desarrollo del primer proyecto. La separación clara respecto al componente de Business Intelligence evita confusiones y permite que cada parte pueda evaluarse de manera independiente.  \n",
    "\n",
    "- **`data/`** \n",
    "\n",
    "  Carpeta dedicada al almacenamiento local de archivos generados o procesados. Se divide en:  \n",
    "\n",
    "  * **`raw/`**: ubicación de los archivos generados directamente por el script de datos sintéticos (CSV o Parquet). Representa el nivel más cercano al origen.  \n",
    "  * **`processed/`**: reservado para transformaciones intermedias, si se requieren. Aunque en este proyecto es probable que no se utilice ampliamente, refleja una práctica común en pipelines de datos.  \n",
    "\n",
    "  Esta separación permite reproducir un flujo tipo medallón (bronze/silver), incluso en un entorno simplificado.  \n",
    "\n",
    "- **`src/`** \n",
    "\n",
    "  Reúne todos los componentes programáticos:  \n",
    "\n",
    "  * **`data_generation/`**: scripts responsables de la creación de datos sintéticos.  \n",
    "  * **`batch_etl/`**: scripts que implementan la migración hacia Azure SQL, representando el proceso batch solicitado.  \n",
    "  * **`api/`**: implementación de la API FastAPI, con controladores, modelos y lógica de conexión a la base de datos.  \n",
    "\n",
    "  Organizar los scripts en carpetas distintas asegura claridad para evaluación y mantenimiento.  \n",
    "\n",
    "- **`docker/`** Contendrá el `Dockerfile`, archivos de configuración necesarios y notas sobre la contenerización de la API. Ubicarlo aquí mantiene separado el aspecto operativo de la lógica de negocio.  \n",
    "\n",
    "- **`docs/`** Espacio para diagramas de arquitectura, decisiones de diseño y cualquier documento complementario que apoye la comprensión del flujo end to end.  \n",
    "\n",
    "- **`.env.example`** Archivo plantilla que define las variables de entorno que la API o los scripts necesitan (cadenas de conexión, claves o parámetros). No contendrá información sensible, pero sirve como guía para reproducir la configuración.  \n",
    "\n",
    "- **`requirements.txt`** Lista mínima de dependencias necesarias para ejecutar este proyecto localmente.\n",
    "\n",
    "**`02_bi_northwind/`**\n",
    "\n",
    "Directorio independiente para el proyecto de Business Intelligence.\n",
    "\n",
    "- **`pbix/`**\n",
    "\n",
    "  Almacena el archivo de Power BI Desktop con el modelo dimensional y el dashboard construidos sobre Northwind.\n",
    "\n",
    "- **`docs/`**\n",
    "\n",
    "  Aquí se incluirán los documentos técnicos complementarios, como el informe justificando el modelo y las decisiones de diseño.\n",
    "\n",
    "- **`notes/`**\n",
    "\n",
    "  Espacio opcional para anotaciones, capturas parciales o materiales que apoyen la documentación.\n",
    "\n",
    "\n",
    "**`README.md`**\n",
    "\n",
    "  Actúa como punto de entrada del repositorio. Proporcionará una vista general, instrucciones de ejecución, arquitectura y navegación por los dos proyectos. Será el documento que un evaluador lea primero, por lo que debe estar alineado con la calidad del resto de la solución.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83748fbb",
   "metadata": {},
   "source": [
    "## **6.3 Justificación de la estructura**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c855b610",
   "metadata": {},
   "source": [
    "Esta organización cumple los siguientes objetivos:\n",
    "\n",
    "- **Claridad**: cada componente tiene un espacio propio, evitando la mezcla de scripts, documentación y artefactos operativos.  \n",
    "\n",
    "- **Reproducibilidad**: permite a un evaluador ejecutar únicamente la parte que desea evaluar sin navegar entre carpetas irrelevantes.  \n",
    "\n",
    "- **Escalabilidad**: si en el futuro se agregan componentes como orquestadores, flujos incrementales o herramientas de monitorización, existe un espacio natural para integrarlos.  \n",
    "\n",
    "- **Separación conceptual**: el proyecto de BI es de naturaleza distinta del de ingeniería de datos; mantenerlos aislados mejora la lectura y la valoración técnica.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f65ead",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ec47f6b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f52f81",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
